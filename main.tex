\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage[margin=0.5in]{geometry}
\setstretch{1.1}

\title{\textbf{Econometrics}}
\date{ }

\begin{document}

\maketitle
\section*{Лекция 1. Общая информация и введение}

\section*{Линейная регрессия и метод наименьших квадратов, МНК(OLS)}
\textbf{Из Марно Вербника}

\begin{itemize}[noitemsep]
    \item $X = x_1, x_2, ... , x_n$ - вектор параметров c размерностью (m, n).
    \item $Y$ - вектор значений c размерностью (n, 1)
\end{itemize}

Вопрос на который отвечает метод наименьших квадратов звучит следующим образом: \textbf{Какая линейная комбинация $X$ с константой дает хорошую аппроксимацию для $Y$?}

\begin{itemize}[noitemsep]
    \item Для того, что бы ответить на этот вопрос запишем произвольную линейную комбинацию: $\hat{\beta_0}+ \hat{\beta_1}x_1 + ..... + \hat{\beta_n}x_n$, где $\hat{\beta_n}$ - константы, которые мы в будующем подберем. 
    \item Разность между наблюдаемым значением и его линейной аппроксимацией: $y_i - [\hat{\beta_0}+ \hat{\beta_1}x_{i1} + ..... + \hat{\beta_n}x_{in}]$
    \item Следовательно мнимизировать будем функцию квадратов разностей $$S(\hat{\beta}) = \sum_{i = 1} ^{n} (y_i - x_{i} ^{'}\hat{\beta})^2$$
    \item Отсюда следует, что решениями проблемы минимизации является одно из двух следующих уравнений:
    $$ b = (\sum _{i= 1} ^n x_i x_i^{'})^{1}  \sum _{i= 1} ^n x_i y_i$$
    
    $$\beta = (X'X)^{-1}(X'Y)$$
\end{itemize}

\textbf{Распределения}

$Z \sim N(0, 1)$

$$\sum Z_i^2 \sim \chi_n^2$$
$E(\chi_n^2) = n, Var(\chi_n^2) = 2n$

$$\frac{Z}{\sqrt{\frac{\chi_n^2}{n}}} \sim t_n$$
$E(t_n) = 0, Var(t_n) = 2n$

$$\frac{\chi_n^2/n}{\chi_m^2/m} \sim F_{n, m}$$
$E(F_{n, m}) = \frac{m}{m-2}$

\section*{Лекция 1.2. Повторение ТВ и МС}

Функия распределения $F_X(x)$СВ X называется: $F_X(x)  = P(X \leq x)$, СВ Х называется непрерывной, если существует функция $f(x)$, такая что $F_X^{'}(x) = f(x)$, где $f(x)$ обладает следующими свойствами: 

$$ f(x) \geq 0 $$ 

$$\int_{-\infty} ^{+\infty} f(x)dx = 1$$ 

$$P(a\leq x \leq b) = \int _{a} ^b f(x)dx $$

МО для дискретной СВ: $E(X) = \sum_{i = 1} ^{N} X_i P_i$, МО для непреревной СВ: $E(X) = \int_{-\infty} ^{+\infty} xf(x)dx$

Дисперсия СВ X: $\sigma^2 = Var(X) = E(X-E(X)) = E^2(X) - E(X^2)$, стандартное отклонение - $\sqrt{\sigma^2}$

Ковариация двух СВ X Y $Cov(X, Y) = E[X-E(X)(Y-E(Y)]$

Коэффициент корреляции $r_{XY} = \frac{Cov(X, Y)}{\sigma_X \sigma_Y}$, со следующими свойствами:

1. $|r_{XY}| \leq 1$, 

2. При $r_{XY} = 0$ линейная связь СВ X и Y отсутствует полностью, 

3. При $r_{XY} = 1$ то между случайными величинами X и Y существует точная линейная связь: Y = aX + b 

\section*{Лекция 2. Линейная регрессия для одного параметра}

Линейный регрессионный анализ объединяет широкий круг задач, связанных с построением зависимостей между двумя переменными: X и Y.
X – независимая, объясняющая, экзогенная переменная, регрессор, regressor,
Y- зависимая, объясняемая, эндогенная переменная, regressand.

$E(Y|X = x) = f(x)$ – уравнение парной регрессии, из которого следующует следующее: $Y_i = E(Y|X= x_i) + \epsilon_i = f(X_i) + \epsilon_i$, где $\epsilon$ стохаистическое возмущение.


$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i, i = 1, ..., n$– линейная регрессионная модель, $\beta_0, \beta_1$ параметры, которые необходимо оценить по выборке. $Y = \beta_0 + \beta_1 X$ - линия теоритической регрессии, а  $\hat{Y} = \hat{\beta_0} + \hat{\beta_11}X$ - линия выборочной регрессии.

\textbf{RSS} - Residual Sum of Squares.
OLS((МНК) основан на минимизации RSS. Таким образом задача OLS сводится к следующему:

$$RSS(\hat{\beta_0}, \hat{\beta_1}) \rightarrow min $$

Формулы для оценок OLS.
$$\hat{\beta_1} = \frac{\sum X_i Y_i - n\bar{X}\bar{Y}}{\sum X_i^2 - n\bar{X^2}} = \frac{\hat{Cov(X, Y)}}{\hat{Var(X)}}$$

$$\hat{\beta_0} = \bar{Y} - \hat{\beta_1} \bar{X}$$

\section*{Лекция 3. Линейная регрессия для одного параметра - дисперсионный анализ}

\textbf{Условия для линейной регрессии с константой}

\begin{enumerate}
    \item $\bar{Y} = \hat{\beta _0} + \hat{\beta_1}\bar{X}$ - Линия регрессии проходит через $\bar{X}, \bar{Y}$
    
    \item $\sum e_i = 0 $ - Отсутствует систематическая ошибка
    
    \item $\sum Y_i = \sum \bar{Y_i}$ - Сумма всех значений $Y$ совпадает с суммой всех выровненных $Y$
    
    \item $\bar{Y} = \hat{Y}$ - Среднее арифметическое по всем значениям $Y$ совпадает со средним арифметическим по всех выровненным $Y$
    
    \item $\sum X_i e_i = 0 $ - Векторы $X$ и $e$ ортогональны
    
    \item $\sum \hat{Y_i} e_i = 0 $ - Векторы $\hat{Y}$ и $e$ ортогональны

\end{enumerate}

\textbf{Sums of squares}

$\sum (Y_i - \bar{Y})^2$ = TSS (Total sum of squares)

$\sum (\hat{Y_i} - \bar{Y})^2$ = ESS (Explained sum of squares)

$\sum e_i^2$ = RSS (Residual sum of squares)

\textbf{TSS = RSS + ESS}

$$R^2 = \frac{ESS}{TSS}  = \frac{\sum (\hat{Y_i} - \bar{Y})^2}{\sum (Y_i - \bar{Y})^2} = \frac{Var({\hat{Y}})}{Var(Y)} = \frac{TSS - RSS}{TSS}$$

$R^2$ является отношением ESS к TSS,(или долей дисперсии Y,объясненной с помощью регрессии). Очевидно, это неотрицательная величина.

\textbf{Критерий лучшей оценки}

Best – это оценки с наименьшей дисперсией в классе всех линейных несмещенных оценок.
$$\sigma_{\hat{\beta_0}}^2 = \sigma_{\epsilon} ^2 \frac{\sum X_i^2}{n\sum x_i^2}, \sigma_{\hat{\beta_1}}^2 = \frac{\sigma_{\epsilon}^2}{\sum x_i^2}$$

\textbf{Оценка дисперсии возмущений}

$$\hat{\sigma_{\epsilon}}^2 = \frac{RSS}{n-2}$$  Является несмещенной оценкой дисперсии возмущений, а также стандартной оценкой регрессии в квадрате. 

\section*{Лекция 4. Теорема Гаусса-Маркова, Классическая линейная регрессия}

$$Y = \beta_0 + \beta_1 X + \epsilon$$

\textbf{Теорема Гаусса - Маркова}

Т.к Х - детерминированный, а $\epsilon$ - случайный вектор, то $\hat{\beta_1}$ - случайная величина. 

\begin{enumerate}[noitemsep]
    \item 1) Если модель $Y = \beta_0 + \beta_1 X + \epsilon$ правильно специфицирована,
    \item 2) $X_i, i= 1, ..., n $ детерминированы и не все равны между собой,
    \item $E(\epsilon_i) = 0 $
    \item $Var(\epsilon_i ) = 0$
    \item $Cov(\epsilon_i, \epsilon_j) = 0$
\end{enumerate}
\textbf{То оценки МНК $\beta_0, \beta_1$ являются BLUE (Best Linear Unbiased Estimator)}

$\epsilon$ - сумма влияния многих факторов, каждый из которых незначительно влияет на $Y$. По Центральной предельной теореме такая случайная величина имеет нормальное распределение.

Если $\epsilon_i$, $i = 1, ..., n$ распределены нормально, то есть $\epsilon_i \sim N(0, \sigma_{\epsilon} ^2 )$, то оценки параметров $\beta_0, \beta_1 $ тоже распределены нормально, причем $\beta_0  \sim N(\beta_0, \frac{\sum X_i^2}{n \sum x_i^2}\sigma _{\epsilon} ^2)$, a $\beta_1  \sim N(\beta_1, \frac{\sigma _{\epsilon} ^2}{ \sum x_i^2})$, где $x_i = X_i - \bar{X}, i = 1, ..., n$

\textbf{Проверка гипотез теория}

\begin{enumerate}[noitemsep]
    \item  Выбора основной и альтернативной гипотезы,
    \item Вычисления некоторой тестовой статистики,
    \item Выбора уровня значимости $\alpha$ (числа между 0 и 1),
    \item Самые распространенные уровни значимости 0.05 и 0.01,
    \item Разбиения множества значений тестовой статистики на две области: там, где
основная гипотеза отвергается и там, где основная гипотеза не отвергается
\end{enumerate}

\textbf{Пример}

Модель: $Y = \beta_0 + \beta_1 X + \epsilon$

Нулевая гипотеза: $H_0: \beta_1 = \beta_1 ^0 $

Двустороння альтернативная гипотеза: $H_1: \beta_1 \neq \beta_1^0$

\begin{enumerate}
    \item Сначала необходимо оценить по n наблюдениям модель: $\hat{Y} = \beta_0 + \beta_1 X + \epsilon$
    \item Если нулевая гипотеза не отвергается, то тестовая статистика $t = \frac{\hat{\beta_1} - \hat{\beta_1^0}}{s.e. (\hat{\beta_1})} \sim t(n-2)$ имеет t - распределение с n-2 степенями свободы. 
    \item Гипотеза отвергается если: $|t| \geq t_{\alpha/2} ^cr$
    \item Если нулевая гипотеза отвергается, то говорят, что коэффициент значим. Если нулевая гипотеза не отвергается, то коэффициент называется незначимым
    \item P – value – минимальный уровень значимости, при котором нулевая гипотеза отвергается. На рисунке это площадь всей заштрихованной области.
    \item В таблице выделены P-value для проверки гипотез о значимости коэффициентов регрессии. Если P-value коэффициента регрессии меньше, чем выбранный уровень значимости α, то нулевая гипотеза отвергается и соответствующий коэффициент является значимым. В приведенном примере при любом разумном уровне значимости константа незначима, а коэффициент наклона значим.
\end{enumerate}

\textbf{ДИ для оценок коэффициентов регрессии. }

Найдем множество всех значений параметра $\beta_1$ , гипотеза о равенстве которым при заданном уровне значимости α и двусторонней альтернативной гипотезе не отвергается.

Гипотеза $H_0: \beta_1 = \beta_1 ^0 $

$t_cr = \frac{\hat{\beta_1} - \beta_1^0}{s.e.(\hat{\beta_1})} \sim t(n-2) $

Следовательно формула ДИ для оценки коэффициента: 

$$\hat{\beta_1} - t_{\alpha/2}^{cr} s.e.(\hat{\beta_1)}\leq \beta_1^0 \leq \hat{\beta_1} + t_{\alpha/2}^{cr} s.e.(\hat{\beta_1)}$$


\end{document}
